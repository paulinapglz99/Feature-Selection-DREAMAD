# -*- coding: utf-8 -*-
"""hyperparameterizador-inador.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GNPKOk9KOnSPRXmeZQIWR4hugLv7T5hS
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
import xgboost as xgb
from sklearn.model_selection import GridSearchCV

#Me voy a inventar un X_train y un y_train
np.random.seed(42)
n_samples = 500
n_features = 10

#X: matriz de caracterÃ­sticas simuladas
X = np.random.randn(n_samples, n_features)
X = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(n_features)])

# y: variable objetivo (clasificaciÃ³n binaria)
y = np.random.randint(0, 2, size=n_samples)

# ðŸ”¹ Dividir en entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the hyperparameter grid
param_grid = {
    'max_depth': [3, 5, 7],
    'learning_rate': [0.1, 0.01, 0.001],
    'subsample': [0.5, 0.7, 1]
}

# Create the XGBoost model object
xgb_model = xgb.XGBClassifier()
xgb_model

# Create the GridSearchCV object
grid_search = GridSearchCV(xgb_model, param_grid, cv=5, scoring='accuracy')

# Fit the GridSearchCV object to the training data
grid_search.fit(X_train, y_train)

#Print the best set of hyperparameters and the corresponding score
print("Best set of hyperparameters: ", grid_search.best_params_)
print("Best score: ", grid_search.best_score_)